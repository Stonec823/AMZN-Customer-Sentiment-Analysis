{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5d769-f192-4a10-a500-40e02eb6f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gcsfs\n",
    "import swifter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a text string by lowercasing, removing punctuation, tokenizing, \n",
    "    removing stopwords, and lemmatizing.\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to preprocess.\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The preprocessed text string, or an empty string if input is invalid.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        # Handle non-string inputs (e.g., NaN or numbers)\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def compute_tfidf_topics(df, text_column, top_n=3, max_features=5000):\n",
    "    \"\"\"\n",
    "    Computes TF-IDF topics for the given text column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing preprocessed text.\n",
    "    text_column : str\n",
    "        The name of the column containing preprocessed text.\n",
    "    top_n : int, optional\n",
    "        The number of top words to extract for each row (default is 3).\n",
    "    max_features : int, optional\n",
    "        The maximum number of features for the TF-IDF vectorizer (default is 5000).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The DataFrame with additional columns for the top N TF-IDF words.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    logging.info(\"Fitting TF-IDF vectorizer...\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    logging.info(\"Extracting top TF-IDF words...\")\n",
    "    top_words = tfidf_df.apply(lambda row: row.nlargest(top_n).index.tolist(), axis=1)\n",
    "    topics_df = pd.DataFrame(top_words.tolist(), columns=[f\"topic_{i+1}\" for i in range(top_n)])\n",
    "    return pd.concat([df, topics_df], axis=1)\n",
    "\n",
    "def compute_sentiment(df, text_column, sentiment_column):\n",
    "    \"\"\"\n",
    "    Computes VADER sentiment scores for a specified text column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing preprocessed text.\n",
    "    text_column : str\n",
    "        The name of the column containing preprocessed text.\n",
    "    sentiment_column : str\n",
    "        The name of the column to store sentiment scores.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The DataFrame with an additional column for sentiment scores.\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    logging.info(\"Computing sentiment scores...\")\n",
    "    df[sentiment_column] = df[text_column].swifter.apply(lambda text: sia.polarity_scores(text)['compound'])\n",
    "    return df\n",
    "\n",
    "def run_pureview_ai(file_path, num_rows=None, chunksize=10000):\n",
    "    \"\"\"\n",
    "    Main function to process data for PureView AI pipeline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the input CSV file.\n",
    "    num_rows : int, optional\n",
    "        Number of rows to read from the file (default is None, meaning all rows).\n",
    "    chunksize : int, optional\n",
    "        Chunk size for batch processing (default is 10000).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The final DataFrame with processed text, TF-IDF topics, and sentiment scores.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting PureView AI pipeline...\")\n",
    "    \n",
    "    if num_rows:\n",
    "        logging.info(f\"Reading the first {num_rows} rows from the file...\")\n",
    "        df = pd.read_csv(file_path, nrows=num_rows)\n",
    "    else:\n",
    "        logging.info(\"Reading the entire file in chunks...\")\n",
    "        df = pd.concat(\n",
    "            pd.read_csv(file_path, chunksize=chunksize), \n",
    "            ignore_index=True\n",
    "        )\n",
    "    \n",
    "    # Step 1: Preprocess the text\n",
    "    logging.info(\"Preprocessing text...\")\n",
    "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Step 2: Drop rows with missing processed text\n",
    "    df = df.dropna(subset=['processed_text'])\n",
    "    \n",
    "    # Step 3: Compute TF-IDF topics\n",
    "    df = compute_tfidf_topics(df, text_column='processed_text')\n",
    "    \n",
    "    # Step 4: Compute sentiment scores\n",
    "    df = compute_sentiment(df, text_column='processed_text', sentiment_column='sentiment_vader')\n",
    "    \n",
    "    logging.info(\"Pipeline completed.\")\n",
    "    return df\n",
    "\n",
    "def output_pureview(df, bucket_name, destination_blob_name):\n",
    "    \"\"\"\n",
    "    Uploads the processed DataFrame to Google Cloud Storage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to upload.\n",
    "    bucket_name : str\n",
    "        The name of the GCS bucket.\n",
    "    destination_blob_name : str\n",
    "        The destination path for the file in the bucket.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    logging.info(\"Uploading results to GCS...\")\n",
    "    local_path = '/tmp/pureview_ai.csv'\n",
    "    df.to_csv(local_path, index=False)\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(local_path)\n",
    "    logging.info(\"Upload completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FILE_PATH = 'gs://amazon-home-and-kitchen/full_train_data.csv'\n",
    "    BUCKET_NAME = 'amazon-home-and-kitchen'\n",
    "    DESTINATION_BLOB_NAME = 'pureview_ai.csv'\n",
    "    NUM_ROWS = 500000\n",
    "    CHUNKSIZE = 10000\n",
    "    \n",
    "    final_df = run_pureview_ai(FILE_PATH, num_rows=NUM_ROWS, chunksize=CHUNKSIZE)\n",
    "    output_pureview(final_df, bucket_name=BUCKET_NAME, destination_blob_name=DESTINATION_BLOB_NAME)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-team184-env-team184-env",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "team184 (Local)",
   "language": "python",
   "name": "conda-env-team184-env-team184-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
