{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67f307-8c6e-4640-86f6-50add5566146",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "/opt/conda/envs/team184-env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre processing now...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt') \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a text string by lowercasing, removing punctuation, tokenizing, \n",
    "    removing stopwords, and lemmatizing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to preprocess.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The preprocessed text string.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def process_dataframe_in_batches(df, num_batches=5):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame in batches, applying text preprocessing to a specified column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing text data to process.\n",
    "    text_column : str\n",
    "        The name of the column containing the raw text to preprocess.\n",
    "    output_column : str\n",
    "        The name of the column where the processed text will be stored.\n",
    "    num_batches : int, optional\n",
    "        The number of batches to split the DataFrame into for processing (default is 5).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The concatenated DataFrame with the processed text in the specified output column.\n",
    "    \"\"\"\n",
    "    # Split the DataFrame into batches\n",
    "    df_batches = np.array_split(df, num_batches)\n",
    "    processed_batches = []\n",
    "    \n",
    "    print('Pre processing now...')\n",
    "    \n",
    "    for i, batch in enumerate(df_batches):\n",
    "        # print(f\"\\n\\tProcessing Batch {i + 1} of {num_batches}...\")\n",
    "        \n",
    "        # Apply text preprocessing to the specified column\n",
    "        batch['processed_text'] = batch['text'].apply(preprocess_text)\n",
    "        processed_batches.append(batch)\n",
    "    \n",
    "    print('Pre processing complete...')\n",
    "    # Concatenate processed batches into a single DataFrame\n",
    "    return pd.concat(processed_batches, ignore_index=True)\n",
    "\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Tokenizes and filters a text string by removing numbers and unwanted words (e.g., stopwords).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to tokenize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string of space-separated tokens containing only alphabetic words\n",
    "        and excluding any stopwords.\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())  # Only keep alphabetic words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "def get_top_words(row):\n",
    "    \"\"\"\n",
    "    Extracts the top 3 words based on their TF-IDF scores from a DataFrame row.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pandas.Series\n",
    "        A row of TF-IDF scores.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A list of the top 3 words (or `None` for missing values if fewer than 3 words are available).\n",
    "    \"\"\"\n",
    "    top_words = row.nlargest(3)  # Get the top 3 words based on TF-IDF score\n",
    "    return top_words.index.tolist() if len(top_words) == 3 else [None, None, None]\n",
    "\n",
    "\n",
    "def get_topics(df):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame to extract topics (top 3 words) from text using TF-IDF.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing a column named 'processed_text' with text data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A new DataFrame containing the original data along with the filtered text and the top 3 topics.\n",
    "        The returned DataFrame includes the following columns:\n",
    "        - `review_id` : Original review ID.\n",
    "        - `main_category` : Main category of the review.\n",
    "        - `title_x` : Title of the review.\n",
    "        - `rating` : Rating of the review.\n",
    "        - `filtered_text` : Processed and filtered text.\n",
    "        - `topic_1`, `topic_2`, `topic_3` : Top 3 topics extracted from the text.\n",
    "    \"\"\"\n",
    "    dfc = df.copy()\n",
    "\n",
    "    # Step 1: Preprocess `processed_text` by removing low-quality words\n",
    "    dfc.loc[:, 'filtered_text'] = dfc['processed_text'].apply(custom_tokenizer)\n",
    "\n",
    "    # Step 2: Define the TF-IDF vectorizer and fit it to the filtered text\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(dfc['filtered_text'])\n",
    "\n",
    "    # Step 3: Create a DataFrame of the TF-IDF scores\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Step 5: Apply the function to each row to get the top 3 words\n",
    "    dfc[['topic_1', 'topic_2', 'topic_3']] = tfidf_df.apply(get_top_words, axis=1, result_type=\"expand\")\n",
    "\n",
    "    return dfc\n",
    "\n",
    "def sentiment_processing(df):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame to compute VADER sentiment scores for the 'filtered_text' column.\n",
    "\n",
    "    This function splits the input DataFrame into batches, calculates the compound sentiment\n",
    "    score for each text in the 'filtered_text' column using the VADER sentiment analyzer, \n",
    "    and returns a concatenated DataFrame with a new column 'sentiment_vader' containing the scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing a column named 'filtered_text', which is expected\n",
    "        to have preprocessed text data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame identical to the input, but with an additional column:\n",
    "        - `sentiment_vader`: The VADER compound sentiment score for each row.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The DataFrame is processed in batches to handle large datasets efficiently.\n",
    "    - The VADER sentiment analyzer computes a compound score, which ranges from -1 (most negative)\n",
    "      to 1 (most positive).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> data = {'filtered_text': ['I love this product!', 'This is terrible.', 'It is okay.']}\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> processed_df = sentiment_processing(df)\n",
    "    >>> print(processed_df)\n",
    "               filtered_text  sentiment_vader\n",
    "    0  I love this product!           0.6369\n",
    "    1     This is terrible.          -0.4767\n",
    "    2          It is okay.           0.0000\n",
    "    \"\"\"\n",
    "\n",
    "    num_batches = 100\n",
    "    df_batches = np.array_split(df, num_batches)\n",
    "\n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    print('Sentiment processing now...')\n",
    "    df_lst = []\n",
    "    for i, batch in enumerate(df_batches):\n",
    "        # print(f\"\\n\\tProcessing Batch {i + 1}...\")\n",
    "        batch['sentiment_vader'] = batch['filtered_text'].apply(lambda text: sia.polarity_scores(text)['compound'])\n",
    "        df_lst.append(batch)\n",
    "    print('Sentiment processing finished...')\n",
    "\n",
    "    concatenated_df = pd.concat(df_lst, ignore_index=True)\n",
    "    return concatenated_df\n",
    "\n",
    "def run_pureview_ai(num_rows):\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    path = f'gs://amazon-home-and-kitchen/full_train_data.csv'\n",
    "    df = pd.read_csv(path, na_values=['—']\n",
    "                     ,nrows=num_rows)\n",
    "    \n",
    "    df=process_dataframe_in_batches(df)\n",
    "    df['review_id'] = df.index \n",
    "    df = df.dropna(subset=['processed_text'])\n",
    "\n",
    "    df_topics = get_topics(df)\n",
    "    final = sentiment_processing(df_topics)    \n",
    "    return final\n",
    "\n",
    "def output_pureview(df):\n",
    "    bucket_name = 'amazon-home-and-kitchen'\n",
    "    destination_blob_name = 'pureview_ai.csv'\n",
    "\n",
    "    # Save DataFrame as CSV locally first\n",
    "    final.to_csv('/tmp/pureview_ai.csv', index=False)\n",
    "\n",
    "    # Initialize a GCS client and upload wwthe file\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename('/tmp/pureview_ai.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_rows=1000000\n",
    "    \n",
    "    df=run_pureview_ai(num_rows)\n",
    "    # output_pureview(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4cd2d8-9159-4b53-a2ba-4c843ac907ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1.68*1000/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41cec8-ae9b-45d6-90ea-df6dcf0dc148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1000*1000"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-team184-env-team184-env",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "team184 (Local)",
   "language": "python",
   "name": "conda-env-team184-env-team184-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
